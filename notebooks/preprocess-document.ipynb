{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1315703d",
   "metadata": {},
   "source": [
    "# Preprocessing Documents and Storing It to The QDRANT\n",
    "\n",
    "In this notebook, we will preprocess the document before storing them into Qdrant vector database. The steps we will take are:\n",
    "1. Load the file into memory\n",
    "2. Chunk the extracted text\n",
    "3. Vectorize the extracted text\n",
    "4. Store the text into qdrant database\n",
    "\n",
    "Let's import important packages first. We will use `langchain-community` package to import `UnstructuredMarkdownLoader` and `PyMuPDFLoader`. If you haven't install the `langchain-community` package, you can use the following command.\n",
    "\n",
    "`pip install -U langchain-community unstructured langchain-pymupdf4llm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4389d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct, HnswConfigDiff\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from langchain_pymupdf4llm import PyMuPDF4LLMLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a675fa14",
   "metadata": {},
   "source": [
    "## Loading File to Memory and Chunk The Extracted Text\n",
    "\n",
    "As the name suggest, we will load a `.md` and `.pdf` file formats. I choose this two files because I have some learning notes that I wrote in the Notion applications. I'm planning to extend the local LLM capabilities to be able to value a stock qualitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "233cdb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "supported_file_formats = [ '.md', '.pdf' ]\n",
    "documents_pth = \"../documents\"\n",
    "\n",
    "document_ls = os.listdir(documents_pth)\n",
    "supported_doc_ls = [ doc for doc in document_ls if doc.endswith(tuple(supported_file_formats)) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc32bdc8",
   "metadata": {},
   "source": [
    "Load the file and split the text using recursive text splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29dbe9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_document(file_path, text_splitter):\n",
    "    \"\"\"Load the document based on the file format.\"\"\"\n",
    "    if file_path.endswith('.md'):\n",
    "        loader = UnstructuredMarkdownLoader(file_path)\n",
    "    elif file_path.endswith('.pdf'):\n",
    "        loader = PyMuPDF4LLMLoader(file_path)\n",
    "    else:\n",
    "        print(f\"File {file_path} isn't supported file format\")\n",
    "        return []\n",
    "\n",
    "    document = loader.load_and_split(text_splitter=text_splitter)\n",
    "\n",
    "    return document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "chunk_documents_ls = []\n",
    "for doc_pth in supported_doc_ls:\n",
    "    chunk_documents_ls.extend(load_and_split_document(os.path.join(documents_pth, doc_pth), text_splitter=text_splitter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4517bee3",
   "metadata": {},
   "source": [
    "## Vectorize The Extracted Text\n",
    "\n",
    "To vectorize the text, we will utilize the `OpenAIEmbeddings` class from openai package. We vectorized it as a batch request using `.embed_documents` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2748779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_ENDPOINT=\"http://localhost:8080/v1\"\n",
    "OPENAI_API_KEY=\"my-openai-api-key\"\n",
    "\n",
    "emb_client = OpenAIEmbeddings(\n",
    "    base_url=OPENAI_ENDPOINT,\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "chunk_content_ls = [ doc.page_content for doc in chunk_documents_ls ]\n",
    "embeddings_documents = emb_client.embed_documents(chunk_content_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a311189c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_documents)  # Check the number of embedded documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2a39d2",
   "metadata": {},
   "source": [
    "## Store The Data Into Qdrant Database\n",
    "\n",
    "1. We connect our client to the Qdrant vector database in localhost. If you did not install the qdrant using docker, you can use \":memory:\" to use qdrant as temporary database.\n",
    "2. We define the collection data structure such as vector size and HNSW configurations.\n",
    "3. We store the data using `PointStruct` class then store it using `.upsert` method.\n",
    "4. To search, we use `.query_points` method and vectorized query as an input.\n",
    "5. To delete a collection, we can use `.delete_collection` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ef7353",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(url=\"http://localhost:6333\")\n",
    "# qdrant_client = QdrantClient(\":memory:\") # use this if you did not install qdrant using docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7826cb",
   "metadata": {},
   "source": [
    "Define a collection and its data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2fbca34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLLECTION_NAME = \"my_collections\"\n",
    "VECTOR_SIZE = 1024 # depends on your embedding model\n",
    "M_VALUE = 32 # The number of edges the nodes will have\n",
    "EF_CONSTRUCT = 128 # The number of neighbours considered during indexing\n",
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(\n",
    "        size=VECTOR_SIZE,\n",
    "        distance=Distance.COSINE,\n",
    "        hnsw_config=HnswConfigDiff(\n",
    "            m=M_VALUE,\n",
    "            ef_construct=EF_CONSTRUCT,\n",
    "            full_scan_threshold=1000\n",
    "        )\n",
    "    ),\n",
    "    on_disk_payload=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3863a9e",
   "metadata": {},
   "source": [
    "Preprocess the data and its payload before inserting to the Qdrant vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b76cf5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_points = []\n",
    "\n",
    "for i, chunk_emb_pair in enumerate(zip(chunk_documents_ls, embeddings_documents)):\n",
    "    chunk, emb = chunk_emb_pair\n",
    "    \n",
    "    temp_payload = {\n",
    "        \"source\": chunk.metadata.get(\"source\", None),\n",
    "        \"text\": chunk.page_content,\n",
    "    }\n",
    "\n",
    "    point = PointStruct(\n",
    "        id=i,\n",
    "        vector=emb,\n",
    "        payload=temp_payload\n",
    "    )\n",
    "    data_points.append(point)\n",
    "\n",
    "len(data_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a49455b",
   "metadata": {},
   "source": [
    "Upsert the data into Qdrant vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "481cd1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "operation_id=0 status=<UpdateStatus.COMPLETED: 'completed'>\n"
     ]
    }
   ],
   "source": [
    "operation_info = qdrant_client.upsert(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    wait=True,\n",
    "    points=data_points\n",
    ")\n",
    "\n",
    "print(operation_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96cc98e",
   "metadata": {},
   "source": [
    "Simple search to check wether the data is already inserted or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef274143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 1, Score: 0.5689739, Payload: ../documents\\Book “The Snowball Warren Buffet and The Business.md\n",
      "Text: Over the long run facts will be more important tha\n",
      "\n",
      "ID: 8, Score: 0.53517836, Payload: ../documents\\Book “Warren Buffet and The Interpretation of Financial report.md\n",
      "Text: In the cash flow statement, you want to make sure \n",
      "\n",
      "ID: 4, Score: 0.49515468, Payload: ../documents\\Book “The Snowball Warren Buffet and The Business.md\n",
      "Text: Companies listed on the stock markets are subject \n",
      "\n",
      "ID: 2, Score: 0.49213064, Payload: ../documents\\Book “The Snowball Warren Buffet and The Business.md\n",
      "Text: Focus on cheap and dislike stock. You can also com\n",
      "\n",
      "ID: 12, Score: 0.48533383, Payload: ../documents\\how-to-identify-superior-stock_warren-buffet.pdf\n",
      "Text: ## **Finding business with moat quickly**\n",
      "\n",
      "You can\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"margin of safety\"\n",
    "\n",
    "search_result = qdrant_client.query_points(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    query=emb_client.embed_query(query),\n",
    "    with_payload=True,\n",
    "    limit=5\n",
    ").points\n",
    "\n",
    "for res in search_result:\n",
    "    print(f\"ID: {res.id}, Score: {res.score}, Payload: {res.payload.get('source')}\")\n",
    "    print(f\"Text: {res.payload['text'][:50]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4ef9cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_client.delete_collection(collection_name=COLLECTION_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
